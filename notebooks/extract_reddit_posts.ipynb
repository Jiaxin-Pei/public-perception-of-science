{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9362f0d7-e72b-4a5e-9a44-54e665d1b068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3789525/752030419.py:13: DtypeWarning: Columns (9) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv('../data/reddit_df.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "279071\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                     | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RS_2018-03.xz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "725it [00:00, 7244.81it/s]\u001b[A\n",
      "1651it [00:00, 8427.71it/s]\u001b[A\n",
      "2641it [00:00, 9098.20it/s]\u001b[A\n",
      "3684it [00:00, 9620.90it/s]\u001b[A\n",
      "4647it [00:00, 9470.66it/s]\u001b[A\n",
      "5640it [00:00, 9621.72it/s]\u001b[A\n",
      "6603it [00:00, 9550.83it/s]\u001b[A\n",
      "7649it [00:00, 9835.04it/s]\u001b[A\n",
      "8634it [00:00, 9508.85it/s]\u001b[A\n",
      "9670it [00:01, 9763.51it/s]\u001b[A\n",
      "10660it [00:01, 9804.20it/s]\u001b[A\n",
      "11671it [00:01, 9893.81it/s]\u001b[A\n",
      "12720it [00:01, 10071.52it/s]\u001b[A\n",
      "13865it [00:01, 10483.88it/s]\u001b[A\n",
      "14997it [00:01, 10728.87it/s]\u001b[A\n",
      "16141it [00:01, 10940.47it/s]\u001b[A\n",
      "17342it [00:01, 11258.24it/s]\u001b[A\n",
      "18505it [00:01, 11365.77it/s]\u001b[A\n",
      "19688it [00:01, 11503.72it/s]\u001b[A\n",
      "20856it [00:02, 11555.59it/s]\u001b[A\n",
      "22013it [00:02, 11557.97it/s]\u001b[A\n",
      "23169it [00:02, 11441.20it/s]\u001b[A\n",
      "24314it [00:02, 11390.01it/s]\u001b[A\n",
      "25466it [00:02, 11423.65it/s]\u001b[A\n",
      "26647it [00:02, 11538.41it/s]\u001b[A\n",
      "27802it [00:02, 11095.83it/s]\u001b[A\n",
      "28916it [00:02, 11067.35it/s]\u001b[A\n",
      "30026it [00:02, 11031.92it/s]\u001b[A\n",
      "31131it [00:02, 10002.29it/s]\u001b[A\n",
      "32150it [00:03, 8088.45it/s] \u001b[A\n",
      "33026it [00:03, 7601.37it/s]\u001b[A\n",
      "34085it [00:03, 8319.82it/s]\u001b[A\n",
      "35184it [00:03, 8996.34it/s]\u001b[A\n",
      "36326it [00:03, 9643.39it/s]\u001b[A\n",
      "37501it [00:03, 10219.55it/s]\u001b[A\n",
      "38684it [00:03, 10673.81it/s]\u001b[A\n",
      "39782it [00:03, 10756.58it/s]\u001b[A\n",
      "41557it [00:04, 10186.55it/s]\u001b[A\n",
      "  0%|                                                     | 0/1 [00:04<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 75\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m zipper\u001b[38;5;241m.\u001b[39mopen(input_path \u001b[38;5;241m+\u001b[39m filename, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrt\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m load_f:\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;66;03m#lines = load_f.readlines()\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m tqdm(load_f):\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;66;03m#line = line.readlines()\u001b[39;00m\n\u001b[1;32m     77\u001b[0m         \u001b[38;5;66;03m#try:\u001b[39;00m\n\u001b[1;32m     78\u001b[0m         line \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(line)\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;66;03m#if 'subreddit' not in line:\u001b[39;00m\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m#    continue\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/lzma.py:212\u001b[0m, in \u001b[0;36mLZMAFile.read1\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    211\u001b[0m     size \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mDEFAULT_BUFFER_SIZE\n\u001b[0;32m--> 212\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer\u001b[38;5;241m.\u001b[39mread1(size)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mread(\u001b[38;5;28mlen\u001b[39m(byte_view))\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/opt/anaconda/lib/python3.11/_compression.py:103\u001b[0m, in \u001b[0;36mDecompressReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         rawblock \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decompressor\u001b[38;5;241m.\u001b[39mdecompress(rawblock, size)\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import lzma\n",
    "import argparse\n",
    "import bz2\n",
    "import re\n",
    "import zstd\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "df = pd.read_csv('../data/reddit_df.csv')\n",
    "\n",
    "subreddits = defaultdict(int)#{'AskWomen':0,'AskMen':0,'asktransgender':0, 'AskWomenOver30':0, 'AskMenOver30':0}\n",
    "domains = {'nature.com':0,'sciencedaily.com':0,'phys.org':0, 'eurekalert.org':0, 'bioengineer.org':0, 'ncbi.nlm.nih.gov':0, 'sciencedirect.com':0, 'scientificamerican.com':0,\n",
    "          'newscientist.com':0, 'sciencemag.org':0, 'iflscience.com':0, 'livescience.com':0, 'psypost.org':0, 'onlinelibrary.wiley.com':0, 'sciencealert.com':0, 'pnas.org': 0,\n",
    "          'sciencenews.org':0, 'news.sciencemag.org':0, 'cell.com': 0, 'medicalxpress.com':0}\n",
    "urls = set(df['url'])\n",
    "print(len(urls))\n",
    "input_path = '/shared/2/datasets/reddit-dump-all/RS/'\n",
    "output_path = '/shared/2/projects/jiaxin/selective-reporting/data/reddit/RS/'\n",
    "if not os.path.exists(output_path):\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "filelist = os.listdir(input_path)\n",
    "existing_months = set()\n",
    "selected_filelist = []\n",
    "for it in filelist:\n",
    "    if re.search('201[8]-03\\.xz',it) and it.split('.')[0] not in existing_months:\n",
    "        existing_months.add(it.split('.')[0])\n",
    "        selected_filelist.append(it)\n",
    "filelist = selected_filelist\n",
    "#filelist = [it for it in filelist if re.search('2019-\\d\\d\\.',it) and ]\n",
    "saved_files = os.listdir(output_path)\n",
    "\n",
    "saved_list = {}\n",
    "for filename in saved_files:\n",
    "    saved_list[filename[:-3]] = 0\n",
    "print(len(filelist))\n",
    "\n",
    "\n",
    "data_dict = {}\n",
    "data_list = []\n",
    "keys = ['author', 'created_utc', 'domain', 'id', 'num_comments', 'score', 'url','over_18', 'permalink','selftext','subreddit', 'title']\n",
    "\n",
    "qs = 0\n",
    "count = 0\n",
    "skipped = 0\n",
    "\n",
    "for filename in tqdm(filelist):\n",
    "    #filename = 'RS_2019-05.xz'\n",
    "    if filename in saved_list:\n",
    "        skipped += 1\n",
    "        print('skip', filename)\n",
    "        print('skipped',skipped)\n",
    "        continue\n",
    "    print(filename)\n",
    "    \n",
    "\n",
    "    #if re.search('201', filename):\n",
    "    #    continue\n",
    "\n",
    "    if re.search('\\.xz', filename):\n",
    "        zipper = lzma\n",
    "    elif re.search('\\.bz2', filename):\n",
    "        zipper = bz2\n",
    "    #elif re.search('\\.zst', filename):\n",
    "    #    zipper = zstd\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "    with zipper.open(input_path + filename, 'rt') as load_f:\n",
    "        #lines = load_f.readlines()\n",
    "        for line in tqdm(load_f):\n",
    "            #line = line.readlines()\n",
    "            #try:\n",
    "            line = json.loads(line)\n",
    "            #if 'subreddit' not in line:\n",
    "            #    continue\n",
    "            \n",
    "            if line['url'] in urls:\n",
    "                new_item = {k:line[k] for k in keys}\n",
    "                #new_item = {'id': line['id'], 'title': line['title'], 'author': line['author'],\n",
    "                #            'created_utc': line['created_utc'], 'subreddit': line['subreddit']}\n",
    "                #data_list.append(json.dumps(new_item))\n",
    "                data_list.append(new_item)\n",
    "                subreddits[line['subreddit']] += 1\n",
    "                    \n",
    "                    #break\n",
    "\n",
    "            #except:\n",
    "            #    continue\n",
    "\n",
    "                \n",
    "    #for key in subreddits:\n",
    "    #    print(key, subreddits[key])\n",
    "        \n",
    "    #break\n",
    "    \n",
    "    \n",
    "    with lzma.open(output_path + filename.split('.')[0] + '.xz', 'wt') as write_f:\n",
    "        #write_f.write('\\n'.join(question_list) + '\\n')\n",
    "        for line in tqdm(data_list):\n",
    "            write_f.writelines(json.dumps(line) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088d66fe-0b6b-47ec-8d7b-e88b0faac5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in subreddits:\n",
    "    print(key, subreddits[key])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35aafa14-9425-48d7-83bc-1b19a047889c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subreddits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8840323-98d9-4a84-bc57-5b64d5d97bf8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'http://blogs.kqed.org/science/2014/04/22/in-california-drought-desperation-may-make-water-flow-uphill/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['url'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a53739af-6ab5-4753-9740-c6082fca0af4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'all_awardings': [],\n",
       " 'archived': False,\n",
       " 'author': 'DanHarkinz',\n",
       " 'author_created_utc': 1535940070,\n",
       " 'author_flair_background_color': None,\n",
       " 'author_flair_css_class': None,\n",
       " 'author_flair_richtext': [],\n",
       " 'author_flair_template_id': None,\n",
       " 'author_flair_text': None,\n",
       " 'author_flair_text_color': None,\n",
       " 'author_flair_type': 'text',\n",
       " 'author_fullname': 't2_16pbcjy1',\n",
       " 'author_patreon_flair': False,\n",
       " 'can_gild': True,\n",
       " 'can_mod_post': False,\n",
       " 'category': None,\n",
       " 'content_categories': None,\n",
       " 'contest_mode': False,\n",
       " 'created_utc': 1551414669,\n",
       " 'distinguished': None,\n",
       " 'domain': 'self.Blackops4',\n",
       " 'edited': False,\n",
       " 'gilded': 0,\n",
       " 'gildings': {},\n",
       " 'hidden': False,\n",
       " 'id': 'aw0kk1',\n",
       " 'is_crosspostable': True,\n",
       " 'is_meta': False,\n",
       " 'is_original_content': False,\n",
       " 'is_reddit_media_domain': False,\n",
       " 'is_robot_indexable': True,\n",
       " 'is_self': True,\n",
       " 'is_video': False,\n",
       " 'link_flair_background_color': '',\n",
       " 'link_flair_css_class': 'discussion',\n",
       " 'link_flair_richtext': [{'e': 'text', 't': 'Discussion'}],\n",
       " 'link_flair_template_id': '1edaea44-20fa-11e8-860a-0eb05fa324e6',\n",
       " 'link_flair_text': 'Discussion',\n",
       " 'link_flair_text_color': 'dark',\n",
       " 'link_flair_type': 'richtext',\n",
       " 'locked': False,\n",
       " 'media': None,\n",
       " 'media_embed': {},\n",
       " 'media_only': False,\n",
       " 'no_follow': False,\n",
       " 'num_comments': 0,\n",
       " 'num_crossposts': 0,\n",
       " 'over_18': False,\n",
       " 'parent_whitelist_status': 'all_ads',\n",
       " 'permalink': '/r/Blackops4/comments/aw0kk1/can_we_get_spawn_delays_back_in_hc_otherwise_just/',\n",
       " 'pinned': False,\n",
       " 'pwls': 6,\n",
       " 'quarantine': False,\n",
       " 'removal_reason': None,\n",
       " 'retrieved_on': 1560105910,\n",
       " 'score': 3,\n",
       " 'secure_media': None,\n",
       " 'secure_media_embed': {},\n",
       " 'selftext': \"I don't know why they did away with this but it took out all the tension of matches with was sometimes fun. B perhaps it was a counter to the excessive camping but honestly I much prefer it to what I call Hardcore Express.\\n\\nIf not they might as well just give us the kill cam and HUD back.\\n\\n\\n\",\n",
       " 'send_replies': True,\n",
       " 'spoiler': False,\n",
       " 'stickied': False,\n",
       " 'subreddit': 'Blackops4',\n",
       " 'subreddit_id': 't5_2untl',\n",
       " 'subreddit_name_prefixed': 'r/Blackops4',\n",
       " 'subreddit_subscribers': 297530,\n",
       " 'subreddit_type': 'public',\n",
       " 'suggested_sort': 'top',\n",
       " 'thumbnail': 'self',\n",
       " 'thumbnail_height': None,\n",
       " 'thumbnail_width': None,\n",
       " 'title': 'Can we get spawn delays back in HC otherwise just give us the Killcam.',\n",
       " 'total_awards_received': 0,\n",
       " 'url': 'https://www.reddit.com/r/Blackops4/comments/aw0kk1/can_we_get_spawn_delays_back_in_hc_otherwise_just/',\n",
       " 'whitelist_status': 'all_ads',\n",
       " 'wls': 6}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ff9a90-f1b5-469d-bbeb-c639cd2681b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
