{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ast import literal_eval\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./data/agg_df_with_full_text.csv', converters={\n",
    "    \"users\": literal_eval,\n",
    "    \"scores\": literal_eval\n",
    "    })\n",
    "# data = pd.read_csv('./data/agg_df.csv', converters={\n",
    "#     \"users\": literal_eval,\n",
    "#     \"scores\": literal_eval\n",
    "#     })\n",
    "rating_key = 'Readability and newsworthiness:::The science news story should be published in the news'\n",
    "dimension_to_id = {dim: j for j,dim in enumerate(set(data['dimension']))}\n",
    "annotator_set = set()\n",
    "item_set = set()\n",
    "for j,row in data.iterrows():\n",
    "    if row['dimension'] == rating_key and len(row['users']) > 1:\n",
    "        annotator_set.update(set(row['users']))\n",
    "        item_set.add(row['instance_id'])\n",
    "#annotator_set = set([id_ for j,row in data.iterrows() for id_ in row['users'] if row['dimension'] == rating_key and len(row['users']) > 1])\n",
    "ann_to_id = {ann: j for j,ann in enumerate(annotator_set)}\n",
    "instance_to_row = {id_: j for j,id_ in enumerate(item_set)}\n",
    "score_vector_set = {}\n",
    "item_to_ann = defaultdict(list)\n",
    "ann_to_item = defaultdict(list)\n",
    "singletons = {}\n",
    "for j,row in data.iterrows():\n",
    "    if row['dimension'] == rating_key:\n",
    "        if len(row['users']) > 1:\n",
    "            for ann,score in zip(row['users'], row['scores']):\n",
    "                score_vector_set[(instance_to_row[row['instance_id']],ann_to_id[ann])] = score\n",
    "                item_to_ann[instance_to_row[row['instance_id']]].append(ann_to_id[ann])\n",
    "                ann_to_item[ann_to_id[ann]].append(instance_to_row[row['instance_id']])\n",
    "        elif len(row['users']) == 1:\n",
    "            singletons[row['instance_id']] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial parameters for item mean, var and ann mean,var\n",
    "item_mu = np.array([0.]*len(instance_to_row))\n",
    "item_sig = np.array([1.]*len(instance_to_row))\n",
    "\n",
    "ann_mu = np.array([0.]*len(ann_to_id))\n",
    "ann_sig = np.array([1.]*len(ann_to_id))\n",
    "\n",
    "# Global scale parameter\n",
    "scale = np.array([1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Likelihood of data fit\n",
    "def py(item_mu, item_var, i):\n",
    "    return np.random.normal(loc=item_mu[i], scale=item_var[i])\n",
    "\n",
    "def px(item_mu, item_var, ann_mu, ann_var, i, j):\n",
    "    # Sample from y\n",
    "    yi = np.random.normal(loc=item_mu[i], scale=item_var[i])\n",
    "    # Sample from a\n",
    "    aj = np.random.normal(loc=item_mu[i], scale=item_var[i])\n",
    "\n",
    "    #Likelihood\n",
    "    mu = yi + aj\n",
    "    return np.random.normal(loc=mu, scale=scale)\n",
    "\n",
    "def norm_pdf(x, mu, sig):\n",
    "    return (1 / np.sqrt(2*np.pi*sig**2)) * np.exp(-0.5*((x - mu) / sig)**2)\n",
    "\n",
    "def log_norm_pdf(x, mu, sig):\n",
    "    return (-0.5 * np.log(2*np.pi*sig**2)) - 0.5*((x - mu) / sig)**2\n",
    "\n",
    "def d_norm_pdf(x, mu, sig):\n",
    "    return (x - mu) / sig**2 * -norm_pdf(x, mu, sig)\n",
    "\n",
    "def max_i(observations, item_mu, ann_mu):\n",
    "    # Collect x_ij, mu_i, and mu_j\n",
    "    ratings = defaultdict(list)\n",
    "    for obs in observations:\n",
    "        ratings[obs[0]].append(ann_mu[obs[1]] - observations[obs])\n",
    "    for i in ratings:\n",
    "        ratings[i] = np.mean(r)\n",
    "\n",
    "    return np.array([ratings[i] for i in range(len(instance_to_row))])\n",
    "\n",
    "def max_j(observations, item_mu, ann_mu):\n",
    "    # Collect x_ij, mu_i, and mu_j\n",
    "    ratings = defaultdict(list)\n",
    "    for obs in observations:\n",
    "        ratings[obs[1]].append(item_mu[obs[0]] - observations[obs])\n",
    "    for j in ratings:\n",
    "        ratings[j] = np.mean(r)\n",
    "\n",
    "    return np.array([ratings[j] for j in range(len(ann_to_id))])\n",
    "\n",
    "def max_sigma(observations, item_mu, ann_mu):\n",
    "    return np.sqrt(np.mean([(observations[obs] - (item_mu[obs[0]] + ann_mu[obs[1]]))**2 for obs in observations]))\n",
    "\n",
    "# def py_given_x_theta(y, x, item_mu, item_sig, ann_mu, ann_sig):\n",
    "#     # Get mean of distributions:\n",
    "#     top_mu = x*item_mu - item_mu*ann_mu\n",
    "#     bot_mu = item_mu + ann_mu\n",
    "\n",
    "#     # get sig of distirbutions\n",
    "#     top_sig = x*item_sig - ann_mu*item_sig + item_mu*ann_sig\n",
    "#     bot_sig = ann_sig\n",
    "\n",
    "#     print(top_mu, bot_mu, top_sig, bot_sig)\n",
    "#     # Calculate values\n",
    "#     return norm_pdf(y, top_mu, top_sig) / norm_pdf(y, bot_mu, bot_sig)\n",
    "\n",
    "\n",
    "def py_given_x_theta(y, x, item_mu, item_sig, ann_mu, ann_sig):\n",
    "    # Get mean of distributions:\n",
    "    num = norm_pdf(x, ann_mu + y, ann_sig) * norm_pdf(y, item_mu, ann_sig)\n",
    "    den = norm_pdf(x, ann_mu + item_mu, ann_sig)\n",
    "    # Calculate values\n",
    "    return num / den\n",
    "\n",
    "\n",
    "def loglikelihood(observations, item_mu, ann_mu, sig):\n",
    "    N = len(observations)\n",
    "    term1 = -N / 2 * np.log(2*np.pi)\n",
    "    term2 = -N / 2 * np.log(sig**2)\n",
    "    term3 = (-1 / (2*sig**2)) * np.sum([(observations[obs] - (item_mu[obs[0]] + ann_mu[obs[1]]))**2 for obs in observations])\n",
    "\n",
    "    return term1 + term2 + term2\n",
    "    # #Factorizes based on obersvations, for continuous rvs the likelihood is the PDF evaluated at X\n",
    "    # ll = 0.\n",
    "    # for obs in observations:\n",
    "    #     x = observations[obs]\n",
    "    #     ll += log_norm_pdf(x, item_mu[obs[0]] + ann_mu[obs[1]], sig)\n",
    "    # return ll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "py_given_x_theta(-5, 3.0, item_mu[0], item_sig[0], ann_mu[694], ann_sig[694])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikelihood(score_vector_set, item_mu, ann_mu, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(instance_to_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get initial parameters for item mean, var and ann mean,var\n",
    "item_mu = np.array([0.]*len(instance_to_row))\n",
    "item_var = np.array([1.]*len(instance_to_row))\n",
    "\n",
    "ann_mu = np.array([0.]*len(ann_to_id))\n",
    "ann_var = np.array([1.]*len(ann_to_id))\n",
    "\n",
    "# Global scale parameter\n",
    "scale = np.array([1.])\n",
    "\n",
    "for i in range(100):\n",
    "    print(loglikelihood(score_vector_set, item_mu, ann_mu, scale))\n",
    "    item_mu = max_i(score_vector_set, item_mu, ann_mu)\n",
    "    ann_mu = max_j(score_vector_set, item_mu, ann_mu)\n",
    "    scale = max_sigma(score_vector_set, item_mu, ann_mu)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loglikelihood(score_vector_set, new_item, new_ann, new_sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [[]]*item_mu.shape[0]\n",
    "for obs in score_vector_set:\n",
    "    ratings[obs[0]].append(ann_mu[obs[1]] - score_vector_set[obs])\n",
    "# for i,r in enumerate(ratings):\n",
    "#     ratings[i] = np.array(r).mean()\n",
    "\n",
    "# np.array(ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = [[]]*item_mu.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try as a Gaussian Mixture model\n",
    "\n",
    "Note that since we are using a lot of Gaussians and the data is sparse, this is prone to singularities (variance goes to 0)\n",
    "\n",
    "See here for some solutions: https://stats.stackexchange.com/questions/219302/singularity-issues-in-gaussian-mixture-model\n",
    "\n",
    "Ideally we will use MAP, for now just reset the mean and variance when singularities occur (and then we can just set them to their proper means later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import ipdb\n",
    "eps = 1e-8\n",
    "def norm_pdf(x, mu, sig):\n",
    "    return stats.norm.pdf(x, mu, sig)\n",
    "    #return (1 / np.sqrt(2*np.pi*sig**2)) * np.exp(-0.5*((x - mu) / sig)**2)\n",
    "    \n",
    "def lognorm_pdf(x, mu, sig):\n",
    "    return -0.5*(np.log(2*np.pi*sig**2) + ((x - mu) / sig)**2)\n",
    "\n",
    "def rijk(x, item_mu, item_sig, ann_mu, ann_sig, p):\n",
    "    term1 = max(p * np.exp(lognorm_pdf(x, item_mu, item_sig)), eps)\n",
    "    term2 = max((1. - p) * np.exp(lognorm_pdf(x, ann_mu, ann_sig)), eps)\n",
    "    \n",
    "#     if np.isnan(term1 / (term1 + term2)) or np.isnan(term2 / (term1 + term2)):\n",
    "#         ipdb.set_trace()\n",
    "    \n",
    "    return [term1 / (term1 + term2), term2 / (term1 + term2)]\n",
    "\n",
    "def e_step(observations, item_mu, item_var, ann_mu, ann_var, p):\n",
    "    rij = defaultdict(lambda: [0.,0.])\n",
    "    for obs in observations:\n",
    "        rij[obs] = rijk(observations[obs], item_mu[obs[0]], item_sig[obs[0]], ann_mu[obs[1]], ann_sig[obs[1]], p[obs[1]])\n",
    "    return rij\n",
    "\n",
    "def m_step(observations, item_mu, ann_mu, rij):\n",
    "    new_mu_i = np.array([0.0]*len(instance_to_row))\n",
    "    new_var_i = np.array([1.0]*len(instance_to_row))\n",
    "\n",
    "    new_mu_j = np.array([0.0]*len(ann_to_id))\n",
    "    new_var_j = np.array([1.0]*len(ann_to_id))\n",
    "\n",
    "    new_p = np.array([0.0]*len(ann_to_id))\n",
    "\n",
    "    # Get new i\n",
    "    for i in range(len(instance_to_row)):\n",
    "        r = np.array([rij[(i,j)][0] for j in item_to_ann[i]])\n",
    "        x = np.array([observations[(i,j)] for j in item_to_ann[i]])\n",
    "        new_mu_i[i] = (r*x).sum() / r.sum()\n",
    "        new_var_i[i] = (r * ((x-new_mu_i[i])**2)).sum() / r.sum()\n",
    "        \n",
    "        if new_var_i[i] <= eps or np.isnan(new_var_i[i]):\n",
    "            # singularity, reset\n",
    "            new_mu_i[i] = np.random.uniform(low=0.0, high=4.0)\n",
    "            new_var_i[i] = np.random.uniform(low=1.0, high=3.0)\n",
    "        \n",
    "        if np.isnan(new_mu_i[i]) or np.isnan(new_var_i[i]):\n",
    "            ipdb.set_trace()\n",
    "            pass\n",
    "\n",
    "    for j in range(len(ann_to_id)):\n",
    "        # Using rij[1] gives 1 - p\n",
    "        new_p[j] = 1 - r.mean()\n",
    "        r = np.array([rij[(i,j)][1] for i in ann_to_item[j]])\n",
    "        x = np.array([observations[(i,j)] for i in ann_to_item[j]])\n",
    "        new_mu_j[j] = (r*x).sum() / r.sum()\n",
    "        new_var_j[j] = (r * ((x-new_mu_j[j])**2)).sum() / r.sum()\n",
    "        \n",
    "        if new_var_j[j] <= eps or np.isnan(new_var_j[j]):\n",
    "            # singularity, reset\n",
    "            new_mu_j[j] = np.random.uniform(low=0.0, high=4.0)\n",
    "            new_var_j[j] = np.random.uniform(low=1.0, high=3.0)\n",
    "            new_p[j] = 0.5\n",
    "        \n",
    "        if np.isnan(new_mu_j[j]) or np.isnan(new_var_j[j]) or np.isnan(new_p[j]):\n",
    "            ipdb.set_trace()\n",
    "            pass\n",
    "\n",
    "    return new_mu_i, new_var_i, new_mu_j, new_var_j, new_p\n",
    "\n",
    "def loglikelihood(observations, item_mu, item_sig, ann_mu, ann_sig, p):\n",
    "    return np.array([np.log(p[obs[1]]) + lognorm_pdf(observations[obs], item_mu[obs[0]], item_sig[obs[0]]) + np.log((1-p[obs[1]])) \n",
    "            + lognorm_pdf(observations[obs], ann_mu[obs[1]], ann_sig[obs[1]]) for obs in observations])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LL: -1.1114188006531645e+18:   8%|██████████                                                                                                            | 17/200 [00:02<00:24,  7.48it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m rij \u001b[38;5;241m=\u001b[39m e_step(score_vector_set, item_mu, item_sig, ann_mu, ann_sig, p)\n\u001b[1;32m     15\u001b[0m item_mu_new, item_sig_new, ann_mu_new, ann_sig_new, p_new \u001b[38;5;241m=\u001b[39m m_step(score_vector_set, item_mu, ann_mu, rij)\n\u001b[0;32m---> 16\u001b[0m ll_new_all \u001b[38;5;241m=\u001b[39m \u001b[43mloglikelihood\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore_vector_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_mu_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem_sig_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mann_mu_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mann_sig_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp_new\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m ll_new \u001b[38;5;241m=\u001b[39m ll_new_all\u001b[38;5;241m.\u001b[39msum()\n\u001b[1;32m     18\u001b[0m pbar\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLL: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mll_new\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[18], line 72\u001b[0m, in \u001b[0;36mloglikelihood\u001b[0;34m(observations, item_mu, item_sig, ann_mu, ann_sig, p)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloglikelihood\u001b[39m(observations, item_mu, item_sig, ann_mu, ann_sig, p):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mlog(p[obs[\u001b[38;5;241m1\u001b[39m]]) \u001b[38;5;241m+\u001b[39m lognorm_pdf(observations[obs], item_mu[obs[\u001b[38;5;241m0\u001b[39m]], item_sig[obs[\u001b[38;5;241m0\u001b[39m]]) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mp[obs[\u001b[38;5;241m1\u001b[39m]])) \n\u001b[1;32m     73\u001b[0m             \u001b[38;5;241m+\u001b[39m lognorm_pdf(observations[obs], ann_mu[obs[\u001b[38;5;241m1\u001b[39m]], ann_sig[obs[\u001b[38;5;241m1\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m observations])\n",
      "Cell \u001b[0;32mIn[18], line 72\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloglikelihood\u001b[39m(observations, item_mu, item_sig, ann_mu, ann_sig, p):\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mlog(p[obs[\u001b[38;5;241m1\u001b[39m]]) \u001b[38;5;241m+\u001b[39m lognorm_pdf(observations[obs], item_mu[obs[\u001b[38;5;241m0\u001b[39m]], \u001b[43mitem_sig\u001b[49m\u001b[43m[\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m+\u001b[39m np\u001b[38;5;241m.\u001b[39mlog((\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m-\u001b[39mp[obs[\u001b[38;5;241m1\u001b[39m]])) \n\u001b[1;32m     73\u001b[0m             \u001b[38;5;241m+\u001b[39m lognorm_pdf(observations[obs], ann_mu[obs[\u001b[38;5;241m1\u001b[39m]], ann_sig[obs[\u001b[38;5;241m1\u001b[39m]]) \u001b[38;5;28;01mfor\u001b[39;00m obs \u001b[38;5;129;01min\u001b[39;00m observations])\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Get initial parameters for item mean, var and ann mean,var\n",
    "item_mu = np.random.uniform(low=0.0, high=4.0, size=len(instance_to_row))#np.array([2.]*len(instance_to_row))\n",
    "item_sig = np.random.uniform(low=1.0, high=3.0, size=len(instance_to_row))# np.array([1.]*len(instance_to_row))\n",
    "\n",
    "ann_mu = np.random.uniform(low=0.0, high=4.0, size=len(ann_to_id))#np.array([2.]*len(ann_to_id))\n",
    "ann_sig = np.random.uniform(low=1.0, high=3.0, size=len(ann_to_id))#np.array([1.]*len(ann_to_id))\n",
    "\n",
    "p = np.array([0.5]*len(ann_to_id))\n",
    "\n",
    "ll = loglikelihood(score_vector_set, item_mu, item_sig, ann_mu, ann_sig, p)\n",
    "\n",
    "with tqdm(range(200)) as pbar:\n",
    "    for n in pbar:\n",
    "        rij = e_step(score_vector_set, item_mu, item_sig, ann_mu, ann_sig, p)\n",
    "        item_mu_new, item_sig_new, ann_mu_new, ann_sig_new, p_new = m_step(score_vector_set, item_mu, ann_mu, rij)\n",
    "        ll_new_all = loglikelihood(score_vector_set, item_mu_new, item_sig_new, ann_mu_new, ann_sig_new, p_new)\n",
    "        ll_new = ll_new_all.sum()\n",
    "        pbar.set_description(f\"LL: {ll_new}\")\n",
    "    #     if abs(ll_new - ll) < 0.01:\n",
    "    #         break\n",
    "        ll = ll_new\n",
    "        item_mu = item_mu_new\n",
    "        item_sig = item_sig_new\n",
    "        ann_mu = ann_mu_new\n",
    "        ann_sig = ann_sig_new\n",
    "        p = p_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31.482863325554483"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ll_new_all.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1020, 977)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(score_vector_set.keys())[5804]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39894228.04014329\n",
      "17.501742210747693\n"
     ]
    }
   ],
   "source": [
    "print(np.exp(lognorm_pdf(list(score_vector_set.values())[5804], item_mu[1020], item_sig[1020])))\n",
    "print(lognorm_pdf(list(score_vector_set.values())[5804], ann_mu[977], ann_sig[977]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39894228.04014323\n",
      "39894228.04014323\n"
     ]
    }
   ],
   "source": [
    "print(norm_pdf(list(score_vector_set.values())[5804], item_mu[1020], item_sig[1020]))\n",
    "print(norm_pdf(list(score_vector_set.values())[5804], ann_mu[977], ann_sig[977]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(score_vector_set.values())[5804]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0000000000000004"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ann_mu[977]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3.0, 4.0, 4.0, 3.0, 3.0]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[score_vector_set[(i,977)] for i in ann_to_item[977]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.457046178500062"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_mu = [np.mean([score_vector_set[(i,j)] for j in item_to_ann[i]]) for i in range(len(instance_to_row))]\n",
    "abs(item_mu - all_mu).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.999999929278521"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(item_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.44195984, 0.25000018, 0.79696567, ..., 0.4       , 0.79999997,\n",
       "       0.6       ])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_mu[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean([score_vector_set[obs] for obs in score_vector_set if obs[0] == 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[score_vector_set[obs] for obs in score_vector_set if obs[0] == 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_to_ann[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_mu[245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmin(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "([score_vector_set[obs] for obs in score_vector_set if obs[1] == 245])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_to_item[245]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sig[1390]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[score_vector_set[obs] for obs in score_vector_set if obs[0] == 1390]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_sig[498]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ann_to_item[1130]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_averages = []\n",
    "for i in range(len(instance_to_row)):\n",
    "    x = np.array([score_vector_set[(i,j)] for j in item_to_ann[i]])\n",
    "    base_averages.append(x.mean())\n",
    "base_averages = np.array(base_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(base_averages - item_mu).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "science-news-values",
   "language": "python",
   "name": "science-news-values"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
